Step 1 – Prepare the MinIO warehouse
1.	Open the MinIO web console.
2.	Create a bucket called iceberg-warehouse. This will be our Iceberg warehouse path.
3.	Note the connection details you’ll use from Flink and Trino:
o	Endpoint: http://minio:9000
o	Access key: minioadmin
o	Secret key: minioadmin
These values are just demo defaults; in a real deployment, you’d use secure credentials.
 
Step 2 – Configure an Iceberg catalog in the streaming engine
1.	Open your Flink SQL client (or the SQL client you’re using).
2.	Create an Iceberg catalog pointing to MinIO using the S3-compatible endpoint:
3.	CREATE CATALOG lakehouse WITH (
4.	  'type' = 'iceberg',
5.	  'catalog-type' = 'hadoop',
6.	  'warehouse' = 's3a://iceberg-warehouse/',
7.	  's3.endpoint' = 'http://minio:9000',
8.	  's3.access-key' = 'minioadmin',
9.	  's3.secret-key' = 'minioadmin',
10.	  's3.path-style-access' = 'true'
11.	);
12.	Switch to this catalog and a default database:
13.	USE CATALOG lakehouse;
14.	CREATE DATABASE IF NOT EXISTS demo;
15.	USE demo;
Now the streaming engine treats MinIO as the backing store for Iceberg tables.
 
Step 3 – Create Iceberg tables with upsert semantics
We’ll persist two canonical tables: customers_canonical and orders_canonical.
1.	Define the customers_canonical table:
2.	CREATE TABLE customers_canonical (
3.	  customer_id      INT,
4.	  full_name        STRING,
5.	  primary_email    STRING,
6.	  country_code     STRING,
7.	  created_at       TIMESTAMP(3),
8.	  updated_at       TIMESTAMP(3),
9.	  is_deleted       BOOLEAN,
10.	  PRIMARY KEY (customer_id) NOT ENFORCED
11.	) WITH (
12.	  'connector' = 'iceberg',
13.	  'format-version' = '2'
14.	);
The PRIMARY KEY (customer_id) tells the engine to treat writes as upserts, so each customer ends up as a single latest row, even after many updates.
15.	Define the orders_canonical table similarly:
16.	CREATE TABLE orders_canonical (
17.	  order_id         BIGINT,
18.	  customer_id      INT,
19.	  total_amount     DECIMAL(18,2),
20.	  currency_code    STRING,
21.	  status           STRING,
22.	  created_at       TIMESTAMP(3),
23.	  updated_at       TIMESTAMP(3),
24.	  is_deleted       BOOLEAN,
25.	  PRIMARY KEY (order_id) NOT ENFORCED
26.	) WITH (
27.	  'connector' = 'iceberg',
28.	  'format-version' = '2'
29.	);
30.	Now, connect these Iceberg tables to your streaming sources. For example, assuming customers_stream and orders_stream are CDC-backed source tables:
31.	INSERT INTO customers_canonical
32.	SELECT
33.	  customer_id,
34.	  full_name,
35.	  primary_email,
36.	  country_code,
37.	  created_at,
38.	  updated_at,
39.	  is_deleted
40.	FROM customers_stream;
41.	INSERT INTO orders_canonical
42.	SELECT
43.	  order_id,
44.	  customer_id,
45.	  total_amount,
46.	  currency_code,
47.	  status,
48.	  created_at,
49.	  updated_at,
50.	  is_deleted
51.	FROM orders_stream;
Let the streaming job run for a minute so it writes a few snapshots into the Iceberg warehouse.
 
Step 4 – Configure Trino’s Iceberg catalog against MinIO
Now we want Trino to see the same Iceberg tables.
1.	On the Trino coordinator, open the etc/catalog directory and create a file named lakehouse.properties:
2.	connector.name=iceberg
3.	iceberg.catalog.type=hadoop
4.	hive.metastore=file
5.	iceberg.file-format=PARQUET
6.	
7.	fs.native-s3.enabled=true
8.	s3.endpoint=http://minio:9000
9.	s3.path-style-access=true
10.	s3.aws-access-key=minioadmin
11.	s3.aws-secret-key=minioadmin
12.	
13.	iceberg.warehouse=s3a://iceberg-warehouse/
The exact properties vary by environment, but the key point is: both Flink and Trino point to the same warehouse and S3 endpoint.
14.	Restart Trino so it picks up the new catalog.
 
Step 5 – Query the canonical tables in Trino
1.	From your terminal, start the Trino CLI:
2.	trino --catalog lakehouse --schema demo
3.	List the tables:
4.	SHOW TABLES;
You should see:
o	customers_canonical
o	orders_canonical
5.	Run a simple query:
6.	SELECT customer_id, full_name, primary_email
7.	FROM customers_canonical
8.	ORDER BY customer_id
9.	LIMIT 10;
You’re now querying the same Iceberg tables that your streaming job is continuously updating.
 
Step 6 – Time travel with snapshots for debugging
Iceberg tracks each change as a snapshot, and Trino exposes metadata tables like $snapshots for each Iceberg table.
1.	Inspect snapshots for customers_canonical:
2.	SELECT snapshot_id, committed_at, operation
3.	FROM "customers_canonical$snapshots"
4.	ORDER BY committed_at DESC;
Note the latest snapshot_id and at least one older ID.
5.	Suppose you want to see the table before a buggy deployment changed some logic. Use version travel:
6.	SELECT customer_id, full_name, primary_email
7.	FROM customers_canonical
8.	FOR VERSION AS OF <old_snapshot_id>
9.	WHERE customer_id = 1;
Replace <old_snapshot_id> with the BIGINT from the $snapshots table.
10.	Run the same query without FOR VERSION AS OF to compare with the current state. This lets you see exactly how a particular record changed across snapshots—perfect for debugging and audits.
 
Step 7 – Validate ACID and snapshot isolation in practice
To see snapshot isolation in action:
1.	In one terminal, keep a Trino session open and repeatedly run:
2.	SELECT COUNT(*) FROM orders_canonical;
3.	In parallel, go back to the streaming engine and trigger a burst of new orders—either by inserting new rows in the source DB or by a small load script.
4.	Watch Trino’s counts:
o	For each query, Trino reads a consistent snapshot of the table.
o	You’ll see the count jump from one stable number to another, but you’ll never see half-applied data.
This is Iceberg’s ACID behavior and snapshot isolation in action: each commit produces a new snapshot, and Trino queries always bind to a specific snapshot, not to “in-progress” files.
