Step 1 – Start the local stack
1.	On the terminal, run your docker-compose up or the script that starts Postgres, Kafka, Schema Registry, and your streaming SQL service.
2.	Wait until all containers report “healthy” or “running.”
3.	In a browser tab, open the UI for your SQL engine (for example, Flink SQL web UI or ksqlDB CLI/GUI).
Step 2 – Seed initial customer data
1.	Open a SQL client connected to Postgres.
2.	Create a simple customers table if it doesn’t exist:
	CREATE TABLE customers (
	  customer_id   INT PRIMARY KEY,
	  first_name    VARCHAR(100),
	  last_name     VARCHAR(100),
	  email         VARCHAR(255),
	  country       VARCHAR(50)
	);
10.	Insert a few rows:
	INSERT INTO customers (customer_id, first_name, last_name, email, country) VALUES
	(1, 'Ana',  'Santos', 'ana@example.com', 'ES'),
	(2, 'Bruno','Klein',  'bruno@example.com', 'DE'),
	(3, 'Chloe','Nguyen','chloe@example.com', 'FR');
15.	Confirm that CDC is working by checking the corresponding Kafka topic (for example customers) in your Kafka UI or CLI—you should see events with before and after payloads.
Step 3 – Introduce a schema change on purpose
1.	Back in Postgres, add a new column and slightly change the email field:
	ALTER TABLE customers
	  ADD COLUMN full_name TEXT;
	
	UPDATE customers
	SET full_name = first_name || ' ' || last_name;
7.	Simulate a “risky” change: make some new rows have email as NULL, relying on full_name instead:
	INSERT INTO customers (customer_id, first_name, last_name, email, country, full_name) VALUES
	(4, 'Daria', 'Khan', NULL, 'NL', 'Daria Khan');
10.	Check the Kafka topic again: events now include full_name, and some have null email. Our downstream consumers will need to handle both.
Step 4 – Define the streaming source table
1.	In your streaming SQL UI, create a source table over the CDC topic, declaring the key and value mappings. For example (pseudocode-style):
	CREATE TABLE customers_raw (
	  customer_id INT,
	  first_name  STRING,
	  last_name   STRING,
	  email       STRING,
	  country     STRING,
	  full_name   STRING,
	  PRIMARY KEY (customer_id) NOT ENFORCED
	) WITH (
	  'connector' = 'kafka',
	  'topic' = 'customers',
	  'format' = 'avro',
	  'scan.startup.mode' = 'earliest-offset'
	);
16.	Here, customer_id is our upsert key; this lets the engine maintain one latest row per customer instead of duplicates.
Step 5 – Build customers_canonical with casts, renames, and COALESCE
1.	Now define a derived table or sink that applies our canonical rules:
	CREATE TABLE customers_canonical (
	  customer_id     INT,
	  full_name       STRING,
	  primary_email   STRING,
	  country_code    STRING,
	  PRIMARY KEY (customer_id) NOT ENFORCED
	) WITH (
	  'connector' = 'iceberg',
	  'catalog-name' = 'local_iceberg',
	  'warehouse' = 's3://demo-warehouse/'
	);
13.	Then create a continuous insert that transforms the raw stream:
	INSERT INTO customers_canonical
	SELECT
	  customer_id,
	  COALESCE(full_name, first_name || ' ' || last_name) AS full_name,
	  COALESCE(email, customer_id || '@example.local')    AS primary_email,
	  CAST(country AS STRING)                             AS country_code
	FROM customers_raw;
21.	In this query:
o	COALESCE(full_name, first_name || ' ' || last_name) handles old rows without full_name and new rows where full_name is present.
o	COALESCE(email, customer_id || '@example.local') ensures a usable primary_email even when email is null.
o	We standardize country into country_code and keep customer_id as the primary key for upserts.
Step 6 – Verify upserts and correctness
1.	Let the job run for a moment, then query the canonical table from the streaming SQL UI or from Trino:
.	SELECT * FROM customers_canonical ORDER BY customer_id;
3.	Confirm that:
o	Each customer_id appears only once (upserts are working).
o	full_name is populated either from the new column or by concatenating first and last name.
o	primary_email is never null, even for the row where email was missing.
4.	To test updates, go back to Postgres and change a row:
	UPDATE customers
	SET email = 'ana.new@example.com'
	WHERE customer_id = 1;
8.	Re-run the query on customers_canonical. Customer 1’s primary_email should now show the new value—proof that the CDC + streaming SQL + upsert pipeline is updating the canonical view correctly.
