# M1V3 – Streaming SQL Transformations

> **Pre-requisite:** Make sure the stack is running:
> ```bash
> docker compose up -d
> ```
> And the Debezium connector is registered:
> ```bash
curl -X POST -H "Content-Type: application/json" \
  --data @connectors/postgres-cdc-connector.json \
  http://localhost:8083/connectors
> ```

---

## Step 1 – Verify initial customer data

1. **Connect to Postgres** via docker compose:

```bash
docker compose exec postgres psql -U appuser -d appdb
```

2. **Show the customers table:**

```sql
SELECT * FROM customers ORDER BY customer_id;
```

You should see 3 customers (Ana, Bruno, Chloe) with `first_name`, `last_name`, `email`, and `country`.

3. **Exit psql** when done:

```sql
\q
```

---

## Step 2 – Introduce a schema change (add full_name column)

1. **Connect to Postgres:**

```bash
docker compose exec postgres psql -U appuser -d appdb
```

2. **Add a new column and populate it:**

```sql
ALTER TABLE customers ADD COLUMN IF NOT EXISTS full_name TEXT;
```

```sql
UPDATE customers SET full_name = first_name || ' ' || last_name WHERE full_name IS NULL;
```

3. **Verify the change:**

```sql
SELECT customer_id, first_name, last_name, email, full_name FROM customers;
```

Say:
> "We've evolved the schema by adding a `full_name` column. Existing rows now have this column populated. CDC will capture this change."

4. **Exit psql:**

```sql
\q
```

---

## Step 3 – Insert a row with NULL email (simulating schema mismatch)

1. **Connect to Postgres:**

```bash
docker compose exec postgres psql -U appuser -d appdb
```

2. **Insert a new customer with NULL email:**

```sql
INSERT INTO customers (customer_id, first_name, last_name, email, country, full_name)
VALUES (4, 'Daria', 'Khan', NULL, 'NL', 'Daria Khan')
ON CONFLICT (customer_id) DO UPDATE SET email = EXCLUDED.email, full_name = EXCLUDED.full_name;
```

3. **Verify:**

```sql
SELECT * FROM customers ORDER BY customer_id;
```

Notice customer 4 has `email = NULL` but `full_name = 'Daria Khan'`.

Say:
> "This simulates a real-world scenario where some rows have missing fields. Our streaming SQL will need to handle NULLs gracefully."

4. **Exit psql:**

```sql
\q
```

---

## Step 4 – See the CDC messages in Kafka

1. **Consume messages from the customers topic:**

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic appdb.public.customers \
  --from-beginning \
  --max-messages 5
```

2. **Pretty-print a single message:** skip the first 3 snapshot messages

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic appdb.public.customers \
  --from-beginning \
  --max-messages 4 2>/dev/null | tail -1 | jq .
```

Point out:
- The `after` block now includes `full_name`
- Customer 1 has full `"email": null`

Say:
> "CDC captured both the schema evolution (new column) and the data with NULL values. Now we'll use streaming SQL to normalize this."

---

## Step 5 – Create the source table in Flink SQL

1. **Open the Flink SQL client:**

```bash
docker compose exec flink-jobmanager ./bin/sql-client.sh
```

2. **Create the source table over the CDC topic:**

```sql
CREATE TABLE customers_raw (
  customer_id  INT,
  first_name   STRING,
  last_name    STRING,
  email        STRING,
  country      STRING,
  full_name    STRING,
  PRIMARY KEY (customer_id) NOT ENFORCED
) WITH (
  'connector' = 'kafka',
  'topic' = 'appdb.public.customers',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.group.id' = 'flink-customers-consumer',
  'format' = 'debezium-json',
  'scan.startup.mode' = 'latest-offset'
);
```

3. **Verify the table reads data** (trigger an update first from another terminal):

In another terminal, run:
```bash
docker compose exec postgres psql -U appuser -d appdb -c "UPDATE customers SET country = 'ES' WHERE customer_id = 1;"
```

Then back in Flink SQL:
```sql
SELECT * FROM customers_raw;
```

Press `Q` to exit the result view.

Say:
> "Flink SQL reads the CDC topic as a table. The PRIMARY KEY enables upsert semantics—one row per customer."

---

## Step 6 – Build customers_canonical with COALESCE transformations

Now we create a canonical table that handles NULL values and schema differences.

1. **Still in Flink SQL, create an upsert-kafka sink table:**

```sql
CREATE TABLE customers_canonical (
  customer_id    INT,
  full_name      STRING,
  primary_email  STRING,
  country_code   STRING,
  PRIMARY KEY (customer_id) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'customers_canonical',
  'key.format' = 'json',
  'value.format' = 'json',
  'properties.bootstrap.servers' = 'kafka:9092'
);
```

2. **Start the transformation job:**

```sql
INSERT INTO customers_canonical
SELECT
  customer_id,
  COALESCE(full_name, first_name || ' ' || last_name) AS full_name,
  COALESCE(email, CAST(customer_id AS STRING) || '@example.local') AS primary_email,
  country AS country_code
FROM customers_raw;
```

Say:
> "This job continuously transforms raw CDC events into a clean canonical format:
> - `COALESCE(full_name, ...)` handles old rows without `full_name`
> - `COALESCE(email, ...)` ensures `primary_email` is never NULL
> - The upsert-kafka connector maintains one row per `customer_id`"

The job will run in the background. Press `Ctrl+C` to detach (job continues running).

---

## Step 7 – Verify the canonical topic

1. **From the terminal, consume the canonical topic:**

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic customers_canonical \
  --from-beginning \
  --max-messages 5
```

2. **Trigger updates to see transformations in action:**

```bash
docker compose exec postgres psql -U appuser -d appdb -c "UPDATE customers SET email = 'ana.updated@example.com' WHERE customer_id = 1;"
```

3. **Consume again to see the update:**

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic customers_canonical \
  --from-beginning \
  --max-messages 10
```

Say:
> "The canonical topic now has clean, normalized data. Customer 4's `primary_email` shows `4@example.local` instead of NULL."

---

## Step 8 – Query the canonical data from Flink SQL

1. **Open Flink SQL (or use existing session):**

```bash
docker compose exec flink-jobmanager ./bin/sql-client.sh
```

2. **Create a table to read from the canonical topic:**

```sql
CREATE TABLE customers_canonical_read (
  customer_id    INT,
  full_name      STRING,
  primary_email  STRING,
  country_code   STRING,
  PRIMARY KEY (customer_id) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = 'customers_canonical',
  'key.format' = 'json',
  'value.format' = 'json',
  'properties.bootstrap.servers' = 'kafka:9092'
);
```

3. **Query the transformed data:**

```sql
SELECT * FROM customers_canonical_read;
```

Press `Q` to exit.

You should see:
- Each `customer_id` appears once (upsert working)
- `full_name` is populated for all rows
- `primary_email` is never NULL (customer 4 shows `4@example.local`)

---

## Step 9 – Test upsert behavior with more updates

1. **Update a customer in Postgres:**

```bash
docker compose exec postgres psql -U appuser -d appdb -c "UPDATE customers SET full_name = 'Ana Maria Santos' WHERE customer_id = 1;"
```

2. **Back in Flink SQL, query again:**

```sql
SELECT * FROM customers_canonical_read;
```

Customer 1's `full_name` should now show `Ana Maria Santos`.

Say:
> "Updates flow through CDC → Kafka → Flink transformation → canonical topic. The upsert semantics ensure we always see the latest state."

3. **Exit Flink SQL:**

```sql
EXIT;
```

---

## Key Takeaways

| Concept | Implementation |
|---------|----------------|
| Schema evolution | Added `full_name` column; CDC captured it automatically |
| NULL handling | `COALESCE(email, fallback)` ensures no NULLs in output |
| Column derivation | `COALESCE(full_name, first || ' ' || last)` for backward compatibility |
| Upsert semantics | `PRIMARY KEY` + `upsert-kafka` connector = one row per key |
| Continuous transformation | `INSERT INTO ... SELECT` runs as a streaming job |

---

## Quick Reset

To reset and replay the demo:

```bash
docker compose down -v
docker compose up -d
# Wait ~30s for services to start, then re-register Debezium connector:
curl -X POST -H "Content-Type: application/json" \
  --data @connectors/postgres-cdc-connector.json \
  http://localhost:8083/connectors
```

To remove the `full_name` column for a fresh start:

```bash
docker compose exec postgres psql -U appuser -d appdb -c "ALTER TABLE customers DROP COLUMN IF EXISTS full_name;"
docker compose exec postgres psql -U appuser -d appdb -c "DELETE FROM customers WHERE customer_id = 4;"
```
