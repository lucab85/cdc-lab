# M1V2 – End-to-End CDC Demo

> **Pre-requisite:** Make sure the stack is running:
> ```bash
> docker compose up -d
> ```

---

## Step 1 – Change a row in the source database

1. **Connect to Postgres** via docker compose:

```bash
docker compose exec postgres psql -U appuser -d appdb
```

2. **Show the orders table:**

```sql
SELECT order_id, status, order_total
FROM orders
ORDER BY order_id
LIMIT 3;
```

Point out something like: `order_id = 101`, `status = 'SHIPPED'`.

3. **Update just one row:**

```sql
UPDATE orders
SET status = 'DELIVERED'
WHERE order_id = 101;
```

Say:
> "We're pretending a customer's order just got delivered. Now we'll see how this tiny change flows through the stack."

4. **Exit psql** when done:

```sql
\q
```

---

## Step 2 – See the change as a Kafka message

1. **Run a Kafka consumer** on the orders topic (Debezium names it `appdb.public.orders`):

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic appdb.public.orders \
  --from-beginning \
  --property print.key=true \
  --property print.value=true \
  --max-messages 5
```

2. Point at one of the messages for `order_id = 101`. Call out:
   - **The key:** `{"order_id":101}` → this is the order_id.
   - **The value:** a JSON/Avro payload with fields like `before`, `after`, and `op`.

You don't need to explain every field. Just say:
> "The key identifies which order changed. The value has a before/after envelope, so consumers can see what changed."

---

## Step 3 – Inspect the CDC message structure

Since we're using JSON format (no Avro), the schema is embedded in each message rather than stored in Schema Registry.

1. **Pretty-print a single CDC message:**

```bash
docker compose exec kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic appdb.public.orders \
  --from-beginning \
  --max-messages 1 | jq .
```

2. Point out the Debezium envelope structure:
   - **`before`:** the row state before the change (null for inserts/snapshots)
   - **`after`:** the row state after the change
   - **`op`:** operation type (`r` = read/snapshot, `c` = create, `u` = update, `d` = delete)
   - **`source`:** metadata about where the change came from

3. **Verify Schema Registry is available** (even though we're using JSON):

```bash
curl -s http://localhost:8081/subjects | jq .
```

> This returns `[]` because JSON converters don't register schemas. In production with Avro, you'd see subjects like `appdb.public.orders-key` and `appdb.public.orders-value`.

Explain in simple terms:
> "Each CDC message has a `before`/`after` envelope so consumers know exactly what changed. With Avro format, these schemas would be stored in Schema Registry for version control and compatibility checks."

---

## Step 4 – Read the topic as a table in Flink SQL

Now let's see how a streaming engine turns this into a table.

1. **Open the Flink SQL client:**

```bash
docker compose exec flink-jobmanager ./bin/sql-client.sh
```

2. **Create a source table** (you can say "this is already created for you" if pre-baked):

```sql
CREATE TABLE orders_raw (
  order_id     INT,
  customer_id  INT,
  order_total  STRING,
  order_ts     STRING,
  status       STRING,
  PRIMARY KEY (order_id) NOT ENFORCED
) WITH (
  'connector' = 'kafka',
  'topic' = 'appdb.public.orders',
  'properties.bootstrap.servers' = 'kafka:9092',
  'properties.group.id' = 'flink-orders-consumer',
  'format' = 'debezium-json',
  'scan.startup.mode' = 'earliest-offset'
);
```

> **Note:** `order_total` is defined as STRING because Debezium encodes NUMERIC fields as base64 by default. In production, configure `decimal.handling.mode=string` in the connector for human-readable values.

3. **Run a quick query:**

```sql
SELECT * FROM orders_raw WHERE order_id = 101;
```

Press `Q` to exit the result view.

Explain:
> "Flink SQL is reading from the Kafka topic and presenting it as a table. For each order_id, it keeps track of the latest status. Our update to DELIVERED is now visible here as a streaming row."

---

## Step 5 – Sink to an Iceberg table

Next, we write this simple table into Iceberg.

1. **Still in Flink SQL, create the Iceberg catalog:**

```sql
CREATE CATALOG lakehouse WITH (
  'type' = 'iceberg',
  'catalog-type' = 'rest',
  'uri' = 'http://iceberg-rest:8181',
  'warehouse' = 's3://warehouse/',
  's3.endpoint' = 'http://minio:9000',
  's3.access-key-id' = 'minioadmin',
  's3.secret-access-key' = 'minioadmin',
  's3.path-style-access' = 'true'
);
```

```sql
USE CATALOG lakehouse;
```

```sql
CREATE DATABASE IF NOT EXISTS demo;
```

```sql
USE demo;
```

2. **Create the Iceberg table:**

```sql
CREATE TABLE IF NOT EXISTS orders_canonical (
  order_id     INT,
  customer_id  INT,
  status       STRING,
  PRIMARY KEY (order_id) NOT ENFORCED
) WITH (
  'format-version' = '2',
  'write.upsert.enabled' = 'true'
);
```

3. **Start a streaming insert:**

```sql
INSERT INTO orders_canonical
SELECT order_id, customer_id, status
FROM default_catalog.default_database.orders_raw;
```

Explain:
> "Flink takes the stream from Kafka and writes it into an Iceberg table. Iceberg stores the data as files and uses snapshots so each write is atomic."

4. **Exit Flink SQL** (Ctrl+C or type `EXIT;`).

---

## Step 6 – Query the Iceberg table from Trino

Finally, we'll see the same data from Trino.

1. **Open the Trino CLI:**

```bash
docker compose exec trino trino --catalog lakehouse --schema demo
```

2. **Run a query:**

```sql
SELECT order_id, status
FROM orders_canonical
WHERE order_id = 101;
```

You should see `order_id = 101` with `status = 'DELIVERED'`.

Say:
> "This is the exact same order we changed at the beginning. It flowed through Kafka, Schema Registry, Flink SQL, and Iceberg, and now Trino can query it with normal SQL."

3. **Optionally, run a tiny aggregate:**

```sql
SELECT status, COUNT(*)
FROM orders_canonical
GROUP BY status;
```

Explain:
> "From an analyst's perspective, this just looks like a normal table. They don't need to know where Kafka or Schema Registry are – they just query the current snapshot."

4. **Exit Trino** when done:

```sql
quit;
```

---

## Quick Reset

To reset and replay the demo:

```bash
docker compose down -v
docker compose up -d
# Wait ~30s for services to start, then re-register Debezium connector:
curl -X POST -H "Content-Type: application/json" \
  --data @connectors/postgres-cdc-connector.json \
  http://localhost:8083/connectors
```
