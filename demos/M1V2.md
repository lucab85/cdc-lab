Step 1 – Change a row in the source database
1.	Open your SQL client connected to Postgres.
2.	Show the orders table:
SELECT order_id, status, total_amount
FROM orders
ORDER BY order_id
LIMIT 3;
Point out something like: order_id = 1, status = 'PENDING'.
3.	Update just one row:
UPDATE orders
SET status = 'SHIPPED'
WHERE order_id = 1;
Say:
“We’re pretending a customer’s order just got shipped. Now we’ll see how this tiny change flows through the stack.”
 
Step 2 – See the change as a Kafka message
1.	Open a terminal and run a simple Kafka consumer on the orders topic (name may vary, e.g. orders or dbserver1.public.orders):
kafka-console-consumer \
	  --bootstrap-server localhost:9092 \
	  --topic orders \
	  --from-beginning \
	  --property print.key=true \
	  --property print.value=true \
	  --max-messages 5
9.	Point at one of the messages for order_id = 1. Call out:
o	The key: 1 → this is the order_id.
o	The value: a JSON or Avro-looking payload with fields like before, after, and op.
You don’t need to explain every field. Just say:
“The key identifies which order changed. The value has a before/after envelope, so consumers can see what changed.”
 
Step 3 – Check the schema in Schema Registry
1.	In another terminal, list Schema Registry subjects:
	curl http://localhost:8081/subjects
3.	Highlight two entries that match the topic name, for example:
o	orders-key
o	orders-value
4.	Fetch the latest value schema:
	curl http://localhost:8081/subjects/orders-value/versions/latest
6.	Scroll just enough to show:
o	It’s a record with fields like before, after, and op.
o	There’s a version number.
Explain in simple terms:
“Schema Registry keeps the official shape of messages on this topic. It knows all schema versions and can block changes that would break existing consumers.”
 
Step 4 – Read the topic as a table in Flink SQL
Now let’s see how a streaming engine turns this into a table.
1.	Open the Flink SQL client.
2.	Show a very simple source table (you can say “this is already created for you”):
	CREATE TABLE orders_raw (
	  order_id BIGINT,
	  status   STRING
	) WITH (
	  'connector' = 'kafka',
	  'topic' = 'orders',
	  'value.format' = 'debezium-json',
.	  'scan.startup.mode' = 'earliest-offset'
.	);
(The exact options can be pre-baked in your environment; the idea is just “Kafka topic → SQL table”.)
12.	Run a quick query:
	SELECT *
	FROM orders_raw
	WHERE order_id = 1;
Explain:
“Flink SQL is reading from the Kafka topic and presenting it as a table. For each order_id, it keeps track of the latest status. Our update to SHIPPED is now visible here as a streaming row.”
 
Step 5 – Sink to an Iceberg table
Next, we write this simple table into Iceberg.
1.	Still in Flink SQL, show a small Iceberg table:
	CREATE TABLE orders_canonical (
	  order_id BIGINT,
	  status   STRING
	) WITH (
	  'connector' = 'iceberg',
	  'catalog-name' = 'lakehouse'
	);
9.	Start a basic insert:
	INSERT INTO orders_canonical
	SELECT order_id, status
	FROM orders_raw;
Explain:
“Flink takes the stream from Kafka and writes it into an Iceberg table. Iceberg stores the data as files and uses snapshots so each write is atomic.”
 
Step 6 – Query the Iceberg table from Trino
Finally, we’ll see the same data from Trino.
1.	Open Trino (CLI or web UI) using the same Iceberg catalog and schema (for example, catalog lakehouse, schema default):
2.	trino --catalog lakehouse --schema default
3.	Run:
	SELECT order_id, status
	FROM orders_canonical
	WHERE order_id = 1;
You should see order_id = 1 with status = 'SHIPPED'.
Say:
“This is the exact same order we changed at the beginning. It flowed through Kafka, Schema Registry, Flink SQL, and Iceberg, and now Trino can query it with normal SQL.”
3.	Optionally, run a tiny aggregate:
	SELECT status, COUNT(*) 
	FROM orders_canonical
	GROUP BY status;
Explain:
“From an analyst’s perspective, this just looks like a normal table. They don’t need to know where Kafka or Schema Registry are – they just query the current snapshot.”
